<!DOCTYPE html>
<html lang="en">
<!-- 
  path: src/presentation/architecture.html
  description: Technical Documentation - System Design & Engineering Patterns v21.1.
  
  ABSTRACT:
  Detailed engineering overview of the Z-Realism ecosystem. Covers the 
  Hexagonal Architecture, SOLID implementation, and Hardware Resource 
  Management strategies for the doctoral thesis in Informatics Engineering.
  
  ARCHITECTURAL ROLE (Presentation Layer):
  Acts as the technical blueprint documentation. It provides the mathematical 
  and structural justification for the system's modularity and hardware resilience.

  author: Enrique González Gutiérrez <enrique.gonzalez.gutierrez@gmail.com>
-->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ARCHITECTURE | Z-Realism Research Institute</title>

    <!-- FAVICON CONFIGURATION -->
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    
    <!-- Design System & Documentation Styles -->
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/info.css">
    
    <!-- Mermaid.js for architectural diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true, theme:'dark'});</script>
</head>
<body>
    <!-- Persistent Ambient Cyber-Grid -->
    <div class="grid-overlay"></div>

    <!-- 
      DYNAMIC NAVIGATION INJECTION: 
      The <nav> is automatically injected by js/nav.js.
    -->

    <!-- Documentation Container -->
    <main class="info-container container">
        
        <a href="index.html" class="back-link">&larr; BACK TO GATEWAY</a>

        <header class="info-section">
            <span class="info-subtitle">SYSTEM DESIGN & DDD</span>
            <h1 class="info-title">The Engineering Blueprint</h1>
            <div class="info-content">
                <p>
                    Z-Realism v21.1 is architected using the <strong>Hexagonal Architecture</strong> 
                    (Ports & Adapters) pattern as a foundational design decision rather than an 
                    implementation detail. This architectural choice establishes a strict 
                    separation between the core domain model and all external concerns, allowing 
                    the system to evolve without structural erosion.
                </p>
                <p>
                    This decoupling ensures that the core domain logic—the synthesis, transformation, 
                    and evaluation of character visual DNA—remains fully independent of volatile 
                    infrastructure components, such as specific web frameworks (FastAPI), messaging 
                    systems (Redis), or Generative AI libraries (Diffusers). As a result, technological 
                    substitutions can occur without propagating refactors across the codebase.
                </p>
            </div>
        </header>

        <section class="info-section info-content">
            <h2>Hexagonal Implementation</h2>
            <p>
                The Hexagonal implementation is enforced by defining strict <strong>Ports</strong> 
                (Interfaces) within the Domain Layer. These ports express the domain’s intent in 
                purely semantic terms, avoiding any coupling to implementation mechanics such as 
                CUDA kernels, file systems, or network protocols.
            </p>
            <p>
                Infrastructure adapters implement these ports and are injected at runtime, allowing 
                the system to swap concrete implementations without impacting business logic. 
                For example, the inference engine can toggle seamlessly between the 
                <code>StableDiffusionGenerator</code> and a <code>MockGenerator</code> for non-GPU 
                testing, validation pipelines, or continuous integration environments.
            </p>
            <p>
                This behavior adheres strictly to the <strong>Liskov Substitution Principle (SOLID)</strong>, 
                guaranteeing that all adapters remain behaviorally consistent with the expectations 
                of the domain, regardless of performance characteristics or hardware availability.
            </p>

            <!-- Mermaid Diagram: Hexagonal Structure -->
            <div class="info-card">
                <div class="mermaid">
                    graph LR
                    A[Presentation Layer] --> B[Application Layer]
                    B --> C[Domain Layer]
                    C --> D[Infrastructure Layer]
                    
                    subgraph Domain Boundary
                    C{Ports / Interfaces}
                    B(Use Cases / Orchestration)
                    end
                    
                    subgraph Adapters
                    D --> D1[Inference: CUDA]
                    D --> D2[Broker: Redis]
                    D --> D3[CV: Analytics]
                    end
                </div>
            </div>

            <h2>Hardware Tenancy (GPU Mutex)</h2>
            <p>
                To support high-resolution neural inference on consumer-grade research hardware, 
                including laptops equipped with legacy GPUs (GTX 1060 6GB), the architecture 
                introduces a formalized <strong>Hardware Tenancy Protocol</strong>. This protocol 
                treats GPU access as a scarce, exclusive resource rather than an implicitly shared one.
            </p>
            <p>
                Hardware tenancy is enforced via a distributed, Redis-based Mutex implemented at 
                the API Gateway level. This guarantees that only a single inference workload may 
                acquire GPU execution rights at any given time, preventing undefined behavior 
                caused by VRAM contention and CUDA context fragmentation.
            </p>

            <div class="info-card">
                <h4>Resource Management Logic:</h4>
                <ul class="info-list">
                    <li>
                        <strong>Single-Task Mutex:</strong> Enforces exclusive GPU access, preventing 
                        concurrent inference requests that would otherwise trigger OOM (Out of Memory) 
                        crashes or silent tensor corruption.
                    </li>
                    <li>
                        <strong>Aggressive Cache Purging:</strong> The inference worker explicitly 
                        purges VRAM allocations and invokes garbage collection during context switches 
                        between Static Image and Temporal Video diffusion models, maintaining a clean 
                        execution state.
                    </li>
                    <li>
                        <strong>Weight Streaming:</strong> Model layers are dynamically streamed from 
                        System RAM (32GB) to VRAM (6GB) only during active computation cycles, enabling 
                        large model execution under severe memory constraints.
                    </li>
                </ul>
            </div>

            <h2>Asynchronous Task Orchestration</h2>
            <p>
                The lifecycle of a neural transformation within Z-Realism is designed to be entirely 
                asynchronous. Given the inherently high latency of diffusion-based inference, all 
                long-running operations are decoupled from the request-response cycle of the API.
            </p>
            <p>
                The system utilizes <strong>Celery</strong> as the task orchestrator and 
                <strong>Redis</strong> as the message broker, forming a resilient execution backbone. 
                This architecture ensures that the Application Layer remains responsive and stateless 
                while the Infrastructure Layer performs computationally intensive CUDA workloads.
            </p>
            <p>
                Intermediate telemetry, including denoising state, latent previews, and heuristic 
                metrics, is streamed back through the broker, allowing the client laboratory to 
                observe and evaluate the generative process in near real time.
            </p>

            <!-- Mermaid Diagram: Asynchronous Flow -->
            <div class="info-card">
                <div class="mermaid">
                    sequenceDiagram
                    participant UI as Client Laboratory
                    participant API as FastAPI Gateway
                    participant Q as Redis Broker
                    participant W as Inference Worker
                    
                    UI->>API: POST /transform
                    API->>API: Acquire VRAM Mutex
                    API->>Q: Dispatch Task Manifold
                    Q->>W: Pull Neural Task
                    loop Denoising Cycle
                        W->>Q: Publish Visual Telemetry
                        API-->>UI: Poll State / Preview
                    end
                    W->>Q: Push Final Manifold + Metrics
                    API->>UI: Return Candidate Result
                    API->>API: Release VRAM Mutex
                </div>
            </div>
        </section>

    </main>

    <!-- 
      SHARED ARCHITECTURAL COMPONENTS 
    -->
    <script src="js/nav.js"></script>
    <script src="js/footer.js"></script>
    <script src="js/cookie-consent.js"></script>

</body>
</html>
